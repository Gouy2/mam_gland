{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载 Excel 文件\n",
    "excel_path = './beiyou_excel/chaoyang_retrospective_233.xlsx'\n",
    "labels_df = pd.read_excel(excel_path)\n",
    "\n",
    "# 查看标签数据框架\n",
    "print(labels_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy._core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mload_img\u001b[39;00m\n\u001b[1;32m      5\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(load_img)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mload_img\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dcm, load_nii \n",
      "File \u001b[0;32m~/mg/load_img.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnib\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exposure\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/cv2/__init__.py:181\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra Python code for\u001b[39m\u001b[38;5;124m\"\u001b[39m, submodule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV loader: DONE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/cv2/__init__.py:153\u001b[0m, in \u001b[0;36mbootstrap\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelink everything from native cv2 module to cv2 package\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m py_module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m native_module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcv2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m py_module\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28msetattr\u001b[39m(py_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_native\u001b[39m\u001b[38;5;124m\"\u001b[39m, native_module)\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy._core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "import load_img\n",
    "\n",
    "importlib.reload(load_img)\n",
    "\n",
    "from load_img import load_dcm, load_nii \n",
    "\n",
    "# dir = \"./data/chaoyang_huigu/BAILIANDI RCC\"\n",
    "dir = \"./data/chaoyang_huigu/BAILIANDI RMLO\"\n",
    "\n",
    "img_nib ,_,_= load_nii(os.path.join(dir, \"1.nii.gz\"))\n",
    "print(img_nib.shape)\n",
    "print(img_nib)\n",
    "# #查看img_nib中最大值，以及他的位置\n",
    "# print(np.max(img_nib))\n",
    "# print(np.where(img_nib == np.max(img_nib)))\n",
    "\n",
    "# img_dcm = pydicom.dcmread(os.path.join(dir, \"ser97311img00002.dcm\"))\n",
    "img_dcm = load_dcm(os.path.join(dir, \"ser97311img00001.dcm\"))\n",
    "# print(img_dcm.shape)\n",
    "# print(img_dcm)\n",
    "\n",
    "# # 查看 DICOM 图像\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.imshow(img_dcm, cmap='gray')\n",
    "# plt.savefig(\"./image/test20.png\")\n",
    "\n",
    "plt.imshow(img_nib, cmap='gray')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# # 获取当前文件夹中的所有.txt文件\n",
    "# txt_files = glob.glob(\"./image/*.png\")\n",
    "\n",
    "# # 删除每个.txt文件\n",
    "# for file in txt_files:\n",
    "#     os.remove(file)\n",
    "#     print(f\"{file} 已删除\")\n",
    "\n",
    "# #获取文件夹中所有的.pth文件\n",
    "# pth_files = glob.glob(\"./model_save/*.pth\")\n",
    "\n",
    "# for file in pth_files:\n",
    "#     os.remove(file)\n",
    "#     print(f\"{file} 已删除\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/guyang/miniconda3/envs/mg/lib/python3.8/site-packages/pydicom/charset.py:754: UserWarning: Unknown encoding 'ISO 2022 IR 165' - using default encoding instead\n",
      "  _warn_about_invalid_encoding(encoding)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from load_img import load_dcm, load_nii\n",
    "import numpy as np\n",
    "from process_data import process_images_for_patients\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def process_images_for_patients(base_path, target_size,a =0):\n",
    "\n",
    "    # 获取所有病人文件夹\n",
    "    all_folders = sorted([f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))])\n",
    "\n",
    "    # 按病人分组文件夹，每两个文件夹为一个病人\n",
    "    patient_folders = []\n",
    "    for i in range(0, len(all_folders), 2):\n",
    "        \n",
    "        if i + 1 < len(all_folders):\n",
    "            patient_folders.append([all_folders[i], all_folders[i + 1]])\n",
    "\n",
    "    # print(len(patient_folders))\n",
    "\n",
    "    # for _, row in labels_df.iterrows():\n",
    "    #     label = row['N分期']  # 获取病人标签（N分期）\n",
    "        \n",
    "    # 加载每个文件夹中的图像\n",
    "    patient_images = []\n",
    "\n",
    "    for folder_pair in patient_folders:\n",
    "\n",
    "        all_images = []\n",
    "        for folder in folder_pair:\n",
    "            a += 1\n",
    "\n",
    "            dcm_file = None\n",
    "            nii_file = None\n",
    "\n",
    "            # print(folder)\n",
    "\n",
    "            # 查找对应的 .dcm 和 .nii 文件\n",
    "            for file in os.listdir(os.path.join(base_path, folder)):\n",
    "                if file.endswith('.dcm'):\n",
    "                    dcm_file = os.path.join(base_path, folder, file)\n",
    "                elif file.endswith('.nii.gz'):\n",
    "                    nii_file = os.path.join(base_path, folder, file)\n",
    "                           \n",
    "            # print(f\"dcm_file: {dcm_file}, nii_file: {nii_file}\")\n",
    "\n",
    "            if dcm_file and nii_file:\n",
    "                # 读取并处理 .dcm 和 .nii 图像\n",
    "                dcm_image = load_dcm(dcm_file)\n",
    "                nii_mask , top_left, bottom_right = load_nii(nii_file)\n",
    "\n",
    "                dcm_image = dcm_image[top_left[0]:bottom_right[0], top_left[1]:bottom_right[1]]\n",
    "\n",
    "                # 将两个图像相乘\n",
    "                focused_image = dcm_image * nii_mask\n",
    "\n",
    "                focused_image = cv2.resize(focused_image, target_size)\n",
    "                \n",
    "                # npimage = np.array(focused_dcm_image)\n",
    "                # np.savetxt(npimage,f'./txt/{a}.txt')\n",
    "                # print(npimage)\n",
    "                # np.savetxt(f\"./txt/{a}.txt\", npimage, fmt=\"%.1f\", delimiter=\",\")\n",
    "\n",
    "                # save_image(npimage,f\"./image/{a}.png\")\n",
    "                # plt.imshow(focused_dcm_image, cmap='gray')\n",
    "                # plt.savefig(f\"./image/{a}.png\")\n",
    "                \n",
    "\n",
    "                all_images.append(focused_image)\n",
    "\n",
    "            else:\n",
    "                dcm_image = load_dcm(dcm_file)\n",
    "                dcm_image = cv2.resize(dcm_image, target_size)\n",
    "                all_images.append(dcm_image)\n",
    "                        \n",
    "        if len(all_images) == 2:  # 确保每个病人有 2 张图像\n",
    "\n",
    "            # print(all_images[0].shape)\n",
    "            # 将两个图像堆叠在一起\n",
    "            patient_input = np.stack(all_images, axis=0)  # 形状为 (2, 512, 512)\n",
    "            # 追加至列表中\n",
    "            patient_images.append(patient_input)\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping patient {folder_pair} due to missing images\")\n",
    "\n",
    "    return patient_images\n",
    "\n",
    "def save_image(image_array, save_path):\n",
    "    \"\"\"\n",
    "    将灰度图像数组保存为图像文件\n",
    "    :param image_array: 灰度图像数组 (二维数组)\n",
    "    :param save_path: 保存路径\n",
    "    \"\"\"\n",
    "    # 确保数组是灰度图像（二维数组）\n",
    "    if len(image_array.shape) == 2:\n",
    "        # 将 NumPy 数组转换为 PIL 图像\n",
    "        img = Image.fromarray(image_array.astype(np.uint8))  # uint8 类型\n",
    "        img.save(save_path)\n",
    "    else:\n",
    "        print(len(image_array.shape))\n",
    "    \n",
    "\n",
    "base_path = './data/chaoyang_huigu'  # 图像数据的根目录\n",
    "save_path = './image'\n",
    "target_size = (224, 224)  # 目标图像尺寸\n",
    "\n",
    "patient_images = process_images_for_patients(base_path, target_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def cache_dataset(data, cache_path, format='npy'):\n",
    "    \"\"\"\n",
    "    缓存数据集到指定路径\n",
    "    \n",
    "    参数:\n",
    "    data: 要缓存的数据\n",
    "    cache_path: 缓存文件路径\n",
    "    format: 文件格式 ('npy', 'h5', 'pkl', 'joblib')\n",
    "    \"\"\"\n",
    "    cache_path = Path(cache_path)\n",
    "    \n",
    "    if format == 'npy':\n",
    "        np.save(cache_path, np.array(data))\n",
    "\n",
    "    elif format == 'pkl':\n",
    "        import pickle\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    elif format == 'joblib':\n",
    "        from joblib import dump\n",
    "        dump(data, cache_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {format}\")\n",
    "\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 缓存数据\n",
    "    cache_dataset(patient_images, 'cache/train.npy', format='npy')\n",
    "    \n",
    "    # 之后需要时加载数据\n",
    "    # patient_images = load_cached_dataset('cache/patient_images.npy', format='npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 加载标签数据\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_cached_dataset\u001b[39m(cache_path, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnpy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/pandas/__init__.py:48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     ArrowDtype,\n\u001b[1;32m     51\u001b[0m     Int8Dtype,\n\u001b[1;32m     52\u001b[0m     Int16Dtype,\n\u001b[1;32m     53\u001b[0m     Int32Dtype,\n\u001b[1;32m     54\u001b[0m     Int64Dtype,\n\u001b[1;32m     55\u001b[0m     UInt8Dtype,\n\u001b[1;32m     56\u001b[0m     UInt16Dtype,\n\u001b[1;32m     57\u001b[0m     UInt32Dtype,\n\u001b[1;32m     58\u001b[0m     UInt64Dtype,\n\u001b[1;32m     59\u001b[0m     Float32Dtype,\n\u001b[1;32m     60\u001b[0m     Float64Dtype,\n\u001b[1;32m     61\u001b[0m     CategoricalDtype,\n\u001b[1;32m     62\u001b[0m     PeriodDtype,\n\u001b[1;32m     63\u001b[0m     IntervalDtype,\n\u001b[1;32m     64\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     65\u001b[0m     StringDtype,\n\u001b[1;32m     66\u001b[0m     BooleanDtype,\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     NA,\n\u001b[1;32m     69\u001b[0m     isna,\n\u001b[1;32m     70\u001b[0m     isnull,\n\u001b[1;32m     71\u001b[0m     notna,\n\u001b[1;32m     72\u001b[0m     notnull,\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     Index,\n\u001b[1;32m     75\u001b[0m     CategoricalIndex,\n\u001b[1;32m     76\u001b[0m     RangeIndex,\n\u001b[1;32m     77\u001b[0m     MultiIndex,\n\u001b[1;32m     78\u001b[0m     IntervalIndex,\n\u001b[1;32m     79\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     80\u001b[0m     DatetimeIndex,\n\u001b[1;32m     81\u001b[0m     PeriodIndex,\n\u001b[1;32m     82\u001b[0m     IndexSlice,\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     NaT,\n\u001b[1;32m     85\u001b[0m     Period,\n\u001b[1;32m     86\u001b[0m     period_range,\n\u001b[1;32m     87\u001b[0m     Timedelta,\n\u001b[1;32m     88\u001b[0m     timedelta_range,\n\u001b[1;32m     89\u001b[0m     Timestamp,\n\u001b[1;32m     90\u001b[0m     date_range,\n\u001b[1;32m     91\u001b[0m     bdate_range,\n\u001b[1;32m     92\u001b[0m     Interval,\n\u001b[1;32m     93\u001b[0m     interval_range,\n\u001b[1;32m     94\u001b[0m     DateOffset,\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     to_numeric,\n\u001b[1;32m     97\u001b[0m     to_datetime,\n\u001b[1;32m     98\u001b[0m     to_timedelta,\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     Flags,\n\u001b[1;32m    101\u001b[0m     Grouper,\n\u001b[1;32m    102\u001b[0m     factorize,\n\u001b[1;32m    103\u001b[0m     unique,\n\u001b[1;32m    104\u001b[0m     value_counts,\n\u001b[1;32m    105\u001b[0m     NamedAgg,\n\u001b[1;32m    106\u001b[0m     array,\n\u001b[1;32m    107\u001b[0m     Categorical,\n\u001b[1;32m    108\u001b[0m     set_eng_float_format,\n\u001b[1;32m    109\u001b[0m     Series,\n\u001b[1;32m    110\u001b[0m     DataFrame,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/pandas/core/api.py:47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstruction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flags\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     48\u001b[0m     Grouper,\n\u001b[1;32m     49\u001b[0m     NamedAgg,\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     52\u001b[0m     CategoricalIndex,\n\u001b[1;32m     53\u001b[0m     DatetimeIndex,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatetimes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     62\u001b[0m     bdate_range,\n\u001b[1;32m     63\u001b[0m     date_range,\n\u001b[1;32m     64\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/pandas/core/groupby/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     DataFrameGroupBy,\n\u001b[1;32m      3\u001b[0m     NamedAgg,\n\u001b[1;32m      4\u001b[0m     SeriesGroupBy,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GroupBy\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Grouper\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/pandas/core/groupby/generic.py:77\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     71\u001b[0m     GroupByApply,\n\u001b[1;32m     72\u001b[0m     maybe_mangle_lambdas,\n\u001b[1;32m     73\u001b[0m     reconstruct_func,\n\u001b[1;32m     74\u001b[0m     validate_func_kwargs,\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcom\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     80\u001b[0m     GroupBy,\n\u001b[1;32m     81\u001b[0m     GroupByPlot,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     _transform_template,\n\u001b[1;32m     85\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/pandas/core/frame.py:182\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseFrameAccessor\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstruction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    177\u001b[0m     ensure_wrapped_if_datetimelike,\n\u001b[1;32m    178\u001b[0m     extract_array,\n\u001b[1;32m    179\u001b[0m     sanitize_array,\n\u001b[1;32m    180\u001b[0m     sanitize_masked_array,\n\u001b[1;32m    181\u001b[0m )\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NDFrame\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_key_length\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    185\u001b[0m     DatetimeIndex,\n\u001b[1;32m    186\u001b[0m     Index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     ensure_index_from_sequences,\n\u001b[1;32m    191\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/pandas/core/generic.py:138\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    130\u001b[0m     is_hashable,\n\u001b[1;32m    131\u001b[0m     is_nested_list_like,\n\u001b[1;32m    132\u001b[0m )\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    134\u001b[0m     isna,\n\u001b[1;32m    135\u001b[0m     notna,\n\u001b[1;32m    136\u001b[0m )\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    139\u001b[0m     algorithms \u001b[38;5;28;01mas\u001b[39;00m algos,\n\u001b[1;32m    140\u001b[0m     arraylike,\n\u001b[1;32m    141\u001b[0m     common,\n\u001b[1;32m    142\u001b[0m     indexing,\n\u001b[1;32m    143\u001b[0m     nanops,\n\u001b[1;32m    144\u001b[0m     sample,\n\u001b[1;32m    145\u001b[0m )\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_algos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreplace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m should_use_regex\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExtensionArray\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/pandas/core/indexing.py:77\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstruction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     68\u001b[0m     array \u001b[38;5;28;01mas\u001b[39;00m pd_array,\n\u001b[1;32m     69\u001b[0m     extract_array,\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     72\u001b[0m     check_array_indexer,\n\u001b[1;32m     73\u001b[0m     is_list_like_indexer,\n\u001b[1;32m     74\u001b[0m     is_scalar_indexer,\n\u001b[1;32m     75\u001b[0m     length_of_indexer,\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     78\u001b[0m     Index,\n\u001b[1;32m     79\u001b[0m     MultiIndex,\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     84\u001b[0m         DataFrame,\n\u001b[1;32m     85\u001b[0m         Series,\n\u001b[1;32m     86\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/pandas/core/indexes/api.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_common_type\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m safe_sort\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     Index,\n\u001b[1;32m     20\u001b[0m     _new_Index,\n\u001b[1;32m     21\u001b[0m     ensure_index,\n\u001b[1;32m     22\u001b[0m     ensure_index_from_sequences,\n\u001b[1;32m     23\u001b[0m     get_unanimous_names,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CategoricalIndex\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatetimes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatetimeIndex\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/pandas/core/indexes/base.py:34\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_option\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     NaT,\n\u001b[1;32m     30\u001b[0m     algos \u001b[38;5;28;01mas\u001b[39;00m libalgos,\n\u001b[1;32m     31\u001b[0m     index \u001b[38;5;28;01mas\u001b[39;00m libindex,\n\u001b[1;32m     32\u001b[0m     lib,\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BlockValuesRefs\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjoin\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlibjoin\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     is_datetime_array,\n\u001b[1;32m     38\u001b[0m     no_default,\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:389\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "# 加载标签数据\n",
    "def load_cached_dataset(cache_path, format='npy'):\n",
    "    \"\"\"\n",
    "    加载缓存的数据集\n",
    "    \n",
    "    参数:\n",
    "    cache_path: 缓存文件路径\n",
    "    format: 文件格式 ('npy', 'h5', 'pkl', 'joblib')\n",
    "    \"\"\"\n",
    "    cache_path = Path(cache_path)\n",
    "    \n",
    "    if not cache_path.exists():\n",
    "        raise FileNotFoundError(f\"Cache file not found: {cache_path}\")\n",
    "    \n",
    "    if format == 'npy':\n",
    "        return np.load(cache_path)\n",
    "\n",
    "    elif format == 'pkl':\n",
    "        import pickle\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    elif format == 'joblib':\n",
    "        from joblib import load\n",
    "        return load(cache_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {format}\")\n",
    "    \n",
    "patient_images = load_cached_dataset('cache/train.npy', format='npy')\n",
    "excel_path = './data/beiyou_excel/chaoyang_retrospective_233.xlsx'  # 包含病人姓名和标签的Excel文件路径\n",
    "labels_df = pd.read_excel(excel_path)\n",
    "\n",
    "# 补全标签并构建 images_with_labels 列表\n",
    "images_with_labels = []\n",
    "labels = []\n",
    "for i, patient_input in enumerate(patient_images):\n",
    "    label = labels_df.iloc[i]['N分期']  # 按顺序获取对应的标签\n",
    "\n",
    "    # 如果标签为 NaN，则用均值填充\n",
    "    if pd.isna(label):\n",
    "        label = 1.0\n",
    "\n",
    "    elif label == 2.0 or label == 3.0 :\n",
    "        label = 1.0\n",
    "    \n",
    "\n",
    "    images_with_labels.append((patient_input, label))\n",
    "    labels.append(label)\n",
    "\n",
    "# 输出处理后的标签\n",
    "for i, (imageinput, label) in enumerate(images_with_labels):\n",
    "    print(imageinput.shape)\n",
    "    # break\n",
    "    # print(f\"第 {i+1} 项标签: {label}\")\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "from dataset import ImageDataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "    # transforms.RandomVerticalFlip(),    # 随机垂直翻转\n",
    "    transforms.RandomRotation(20),      # 随机旋转 \n",
    "    # transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  \n",
    "    transforms.Normalize(mean=[0.5, 0.5], std=[0.5, 0.5]),  # 归一化到 [-1, 1]\n",
    "])\n",
    "\n",
    "# 验证集保持原始数据\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5], std=[0.5, 0.5]),  # 与训练集一致的归一化\n",
    "])\n",
    "\n",
    "\n",
    "# 创建数据集实例\n",
    "full_dataset = ImageDataset(images_with_labels)\n",
    "\n",
    "# # 使用 random_split 分割数据集\n",
    "# train_size = int(0.8 * len(full_dataset))  # 80% 训练集\n",
    "# val_size = len(full_dataset) - train_size  # 20% 验证集\n",
    "# train_subset, val_subset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# # 为分割后的子集添加数据增强\n",
    "# train_subset.dataset.transform = train_transform\n",
    "# val_subset.dataset.transform = val_transform\n",
    "\n",
    "# # 创建 DataLoader\n",
    "# train_loader = DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=0)\n",
    "# val_loader = DataLoader(val_subset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "# ## 检查训练集的第一个批次\n",
    "# for inputs, labels in train_loader:\n",
    "#     inputs_numpy = inputs.cpu().numpy()  # 如果在GPU上，先移动到CPU\n",
    "\n",
    "#     # 使用 numpy.array2string 以避免省略和更好地控制格式\n",
    "#     inputs_str = np.array2string(inputs_numpy, separator=', ', threshold=np.inf,precision=2)\n",
    "#     with open(\"output.txt\", 'w') as f:  # 'w' 表示每次打开文件时都会覆盖\n",
    "#         f.write(f\"输入张量的所有值:\\n{inputs_str}\\n\")  # 写入inputs的值\n",
    "\n",
    "#     # print(f\"输入张量形状: {inputs}\")\n",
    "#     print(f\"标签张量形状: {labels}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Subset\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KFold\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 假设 ImageDataset 和 Resnet18_cbam 已定义\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/sklearn/__init__.py:83\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     80\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     81\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     )\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     86\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    130\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/sklearn/utils/__init__.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/sklearn/utils/validation.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_isfinite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FiniteStatus, cy_isfinite\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/sklearn/utils/_array_api.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_array_api_dispatch\u001b[39m(array_api_dispatch):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that array_api_compat is installed and NumPy version is compatible.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    array_api_compat follows NEP29, which has a higher minimum NumPy version than\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    scikit-learn.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/sklearn/utils/fixes.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreadpoolctl\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/scipy/stats/__init__.py:485\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 485\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/scipy/stats/_stats_py.py:46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _mstats_basic \u001b[38;5;28;01mas\u001b[39;00m mstats_basic\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_mstats_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[1;32m     49\u001b[0m                                    siegelslopes)\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/scipy/stats/distributions.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distn_infrastructure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _continuous_distns\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _discrete_distns\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m comb, entr\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# for root finding for continuous distribution ppf, and max likelihood\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# estimation\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m integrate\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/scipy/__init__.py:200\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m submodules:\n\u001b[0;32m--> 200\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_importlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscipy.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/scipy/optimize/__init__.py:418\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_basinhopping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m basinhopping\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linprog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linprog, linprog_verbose_callback\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lsap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_sum_assignment\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_differentialevolution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m differential_evolution\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lsq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m least_squares, lsq_linear\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "from model import Resnet18_cbam\n",
    "\n",
    "# 假设 ImageDataset 和 Resnet18_cbam 已定义\n",
    "full_dataset = ImageDataset(images_with_labels)\n",
    "k_folds = 5\n",
    "batch_size = 4\n",
    "num_epochs = 30\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定义 KFold\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# 存储每个fold的结果\n",
    "fold_results = {}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(full_dataset)):\n",
    "    print(f'Fold {fold + 1}/{k_folds}')\n",
    "    \n",
    "    # 创建子集\n",
    "    train_subset = Subset(full_dataset, train_idx)\n",
    "    val_subset = Subset(full_dataset, val_idx)\n",
    "    \n",
    "    # 添加数据增强\n",
    "    train_subset.dataset.transform = train_transform\n",
    "    val_subset.dataset.transform = val_transform\n",
    "    \n",
    "    # 创建 DataLoader\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # 初始化模型、损失函数和优化器\n",
    "    model = Resnet18_cbam(num_classes=2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        try:\n",
    "            auc = roc_auc_score(all_labels, all_probs)\n",
    "        except ValueError:\n",
    "            auc = 0.5\n",
    "        print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}, AUC={auc:.4f}')\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'best_model_fold_{fold+1}.pth')\n",
    "    \n",
    "    fold_results[fold] = {'val_loss': best_val_loss}\n",
    "    print(f'Fold {fold + 1} 完成，最佳 Val Loss: {best_val_loss:.4f}\\n')\n",
    "\n",
    "# 输出所有fold的结果\n",
    "for fold in fold_results:\n",
    "    print(f'Fold {fold + 1} 最佳 Val Loss: {fold_results[fold][\"val_loss\"]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 训练函数\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # 将数据移动到 GPU 或 CPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # with open(\"debug_log.txt\", \"a\") as f:\n",
    "        #     f.write(f\"Inputs shape: {inputs.shape}\\n\")\n",
    "        #     f.write(f\"Labels shape: {labels.shape}\\n\")\n",
    "        #     f.write(f\"Inputs values: {inputs}\\n\")\n",
    "        #     f.write(f\"Labels values: {labels}\\n\")\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)  # 交叉熵损失函数\n",
    "        # loss = criterion(outputs, labels.view(-1, 1).float())  #   #需要确保标签形状为 (batch_size, 1)，并转换为 float 类型\n",
    "\n",
    "        # 反向传播与优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 记录损失\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 计算准确率\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        # total += labels.size(0)\n",
    "        # correct += (predicted.view(-1) == labels).sum().item()  \n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# 验证函数\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # 将数据移动到 GPU 或 CPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # print(f\"模型输出: {outputs[0]}\")\n",
    "            with open(\"test.txt\", \"a\") as f:\n",
    "                f.write(f\"{outputs}\\n\")\n",
    "\n",
    "            \n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            # loss = criterion(outputs, labels.view(-1, 1).float())\n",
    "\n",
    "            # 记录损失\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 计算准确率\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            # with open(\"test.txt\", \"a\") as f:\n",
    "            #     f.write(f\"{predicted}\\n\")\n",
    "\n",
    "            # total += labels.size(0)\n",
    "            # correct += (predicted.view(-1) == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备：cuda\n",
      "Epoch 1/50\n",
      "Train Loss: 0.6924, Train Acc: 0.5215\n",
      "Val Loss: 0.6946, Val Acc: 0.4894\n",
      "Model saved at 1-th epoch\n",
      "0.48936170212765956\n",
      "Epoch 2/50\n",
      "Train Loss: 0.6580, Train Acc: 0.5968\n",
      "Val Loss: 0.7007, Val Acc: 0.4894\n",
      "0.48936170212765956\n",
      "Epoch 3/50\n",
      "Train Loss: 0.6330, Train Acc: 0.6398\n",
      "Val Loss: 0.6906, Val Acc: 0.6596\n",
      "Model saved at 3-th epoch\n",
      "0.6595744680851063\n",
      "Epoch 4/50\n",
      "Train Loss: 0.5974, Train Acc: 0.6667\n",
      "Val Loss: 0.6738, Val Acc: 0.6170\n",
      "0.6595744680851063\n",
      "Epoch 5/50\n",
      "Train Loss: 0.4874, Train Acc: 0.8011\n",
      "Val Loss: 0.8687, Val Acc: 0.5319\n",
      "0.6595744680851063\n",
      "Epoch 6/50\n",
      "Train Loss: 0.5091, Train Acc: 0.7581\n",
      "Val Loss: 0.8344, Val Acc: 0.6596\n",
      "Model saved at 6-th epoch\n",
      "0.6595744680851063\n",
      "Epoch 7/50\n",
      "Train Loss: 0.4759, Train Acc: 0.7903\n",
      "Val Loss: 1.1858, Val Acc: 0.5319\n",
      "0.6595744680851063\n",
      "Epoch 8/50\n",
      "Train Loss: 0.5378, Train Acc: 0.7097\n",
      "Val Loss: 0.8907, Val Acc: 0.6383\n",
      "0.6595744680851063\n",
      "Epoch 9/50\n",
      "Train Loss: 0.3518, Train Acc: 0.8602\n",
      "Val Loss: 1.0466, Val Acc: 0.6170\n",
      "0.6595744680851063\n",
      "Epoch 10/50\n",
      "Train Loss: 0.4300, Train Acc: 0.8226\n",
      "Val Loss: 0.9176, Val Acc: 0.5532\n",
      "0.6595744680851063\n",
      "Epoch 11/50\n",
      "Train Loss: 0.3179, Train Acc: 0.8817\n",
      "Val Loss: 1.0737, Val Acc: 0.6170\n",
      "0.6595744680851063\n",
      "Epoch 12/50\n",
      "Train Loss: 0.3092, Train Acc: 0.8763\n",
      "Val Loss: 1.0169, Val Acc: 0.6170\n",
      "0.6595744680851063\n",
      "Epoch 13/50\n",
      "Train Loss: 0.2961, Train Acc: 0.8978\n",
      "Val Loss: 1.0669, Val Acc: 0.5532\n",
      "0.6595744680851063\n",
      "Epoch 14/50\n",
      "Train Loss: 0.2115, Train Acc: 0.9140\n",
      "Val Loss: 1.3161, Val Acc: 0.5957\n",
      "0.6595744680851063\n",
      "Epoch 15/50\n",
      "Train Loss: 0.2343, Train Acc: 0.9247\n",
      "Val Loss: 1.5229, Val Acc: 0.4043\n",
      "0.6595744680851063\n",
      "Epoch 16/50\n",
      "Train Loss: 0.2377, Train Acc: 0.9086\n",
      "Val Loss: 1.6089, Val Acc: 0.4468\n",
      "0.6595744680851063\n",
      "Epoch 17/50\n",
      "Train Loss: 0.1595, Train Acc: 0.9624\n",
      "Val Loss: 1.2199, Val Acc: 0.6383\n",
      "0.6595744680851063\n",
      "Epoch 18/50\n",
      "Train Loss: 0.1723, Train Acc: 0.9355\n",
      "Val Loss: 1.1182, Val Acc: 0.4894\n",
      "0.6595744680851063\n",
      "Epoch 19/50\n",
      "Train Loss: 0.1120, Train Acc: 0.9731\n",
      "Val Loss: 1.3972, Val Acc: 0.4894\n",
      "0.6595744680851063\n",
      "Epoch 20/50\n",
      "Train Loss: 0.1677, Train Acc: 0.9462\n",
      "Val Loss: 1.4506, Val Acc: 0.5106\n",
      "0.6595744680851063\n",
      "Epoch 21/50\n",
      "Train Loss: 0.1084, Train Acc: 0.9731\n",
      "Val Loss: 1.1379, Val Acc: 0.5532\n",
      "0.6595744680851063\n",
      "Epoch 22/50\n",
      "Train Loss: 0.0564, Train Acc: 0.9946\n",
      "Val Loss: 1.3133, Val Acc: 0.5319\n",
      "0.6595744680851063\n",
      "Epoch 23/50\n",
      "Train Loss: 0.0857, Train Acc: 0.9785\n",
      "Val Loss: 1.4290, Val Acc: 0.5745\n",
      "0.6595744680851063\n",
      "Epoch 24/50\n",
      "Train Loss: 0.0879, Train Acc: 0.9570\n",
      "Val Loss: 1.0919, Val Acc: 0.5957\n",
      "0.6595744680851063\n",
      "Epoch 25/50\n",
      "Train Loss: 0.0997, Train Acc: 0.9677\n",
      "Val Loss: 1.3203, Val Acc: 0.5532\n",
      "0.6595744680851063\n",
      "Epoch 26/50\n",
      "Train Loss: 0.0675, Train Acc: 0.9839\n",
      "Val Loss: 1.1279, Val Acc: 0.5957\n",
      "0.6595744680851063\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 训练一个 epoch\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     36\u001b[0m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "Cell \u001b[0;32mIn[26], line 30\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# loss = criterion(outputs, labels.view(-1, 1).float())  #   #需要确保标签形状为 (batch_size, 1)，并转换为 float 类型\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 反向传播与优化\u001b[39;00m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 记录损失\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "from model import Resnet18_cbam\n",
    "\n",
    "# 超参数设置\n",
    "model = Resnet18_cbam(num_classes=2)  \n",
    "num_epochs = 50\n",
    "learning_rate = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"使用设备：{device}\")\n",
    "\n",
    "# class_counts = [sum(label == i for label in labels) for i in range(2)]  # 每个类别的样本数量\n",
    "# class_weights = [1.0 / count for count in class_counts]  # 权重为样本数量的倒数\n",
    "# print(f\"每个类别的样本数量：{class_counts}\")\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# 模型、损失函数和优化器\n",
    "model = model.to(device)  # 将模型移动到 GPU 或 CPU\n",
    "criterion = nn.CrossEntropyLoss()  # 多分类交叉熵损失\n",
    "# criterion = nn.BCEWithLogitsLoss()  # 二分类交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # Adam 优化器\n",
    "\n",
    "# 存储训练和验证的结果\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    # 训练一个 epoch\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "\n",
    "    # 在验证集上评估\n",
    "    val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "    # 保存log\n",
    "    with open(\"log.txt\", \"a\") as f:\n",
    "        f.write(f\"Epoch {epoch + 1}/{num_epochs}\\n\")\n",
    "        f.write(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\\n\")\n",
    "        f.write(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\\n\")\n",
    "\n",
    "    # 保存验证集上准确率最高的模型\n",
    "    if epoch == 0 or val_acc > 0.65:\n",
    "        torch.save(model.state_dict(), f\"./model_save/epoch{epoch + 1}_model.pth\")\n",
    "        print(f\"Model saved at {epoch + 1}-th epoch\")\n",
    "\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(max(history[\"val_acc\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N分期（四分类）分类器,初步结果：\n",
    "\n",
    "数据预处理：将nii图像做距离变换，呈现聚焦效果，后与dcm图像做点乘\n",
    "输入：两张dcm图像组成的双通道，shape:(2,512,512)\n",
    "模型：Resnet18 + 注意力模块\n",
    "\n",
    "训练集+验证集：朝阳回顾_233\n",
    "验证集最大准确率：62%\n",
    "\n",
    "测试集：朝阳前瞻_190\n",
    "测试集最大准确率：52%\n",
    "AUC最大值：0.57\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
