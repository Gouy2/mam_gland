{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载 Excel 文件\n",
    "excel_path = './data/beiyou_excel/chaoyang_retrospective_233.xlsx'\n",
    "labels_df = pd.read_excel(excel_path)\n",
    "\n",
    "# 查看标签数据框架\n",
    "print(labels_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mload_img\u001b[39;00m\n\u001b[1;32m      5\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(load_img)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mload_img\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dcm, load_nii \n",
      "File \u001b[0;32m~/mg/load_img.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnib\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/nibabel/__init__.py:41\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124mQuickstart\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m==========\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124mFor more detailed information see the :ref:`manual`.\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# module imports\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze \u001b[38;5;28;01mas\u001b[39;00m ana\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ecat, imagestats, mriutils\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nifti1 \u001b[38;5;28;01mas\u001b[39;00m ni1\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/nibabel/analyze.py:88\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrayproxy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrayProxy\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marraywriters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrayWriter, WriterError, get_slope_inter, make_array_writer\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatteryrunners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Report\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.8/site-packages/nibabel/arrayproxy.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m openers\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfileslice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m canonical_slicers, fileslice\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvolumeutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_read_scaling, array_from_file\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"This flag controls whether a new file handle is created every time an image\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mis accessed through an ``ArrayProxy``, or a single file handle is created and\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mused for the lifetime of the ``ArrayProxy``. It should be set to one of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mraised.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:839\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:934\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1033\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "import load_img\n",
    "\n",
    "importlib.reload(load_img)\n",
    "\n",
    "from load_img import load_dcm, load_nii \n",
    "\n",
    "# dir = \"./data/chaoyang_huigu/BAILIANDI RCC\"\n",
    "dir = \"./data/chaoyang_huigu/BAILIANDI RMLO\"\n",
    "\n",
    "img_nib ,_,_= load_nii(os.path.join(dir, \"1.nii.gz\"))\n",
    "print(img_nib.shape)\n",
    "print(img_nib)\n",
    "# #查看img_nib中最大值，以及他的位置\n",
    "# print(np.max(img_nib))\n",
    "# print(np.where(img_nib == np.max(img_nib)))\n",
    "\n",
    "# img_dcm = pydicom.dcmread(os.path.join(dir, \"ser97311img00002.dcm\"))\n",
    "img_dcm = load_dcm(os.path.join(dir, \"ser97311img00001.dcm\"))\n",
    "# print(img_dcm.shape)\n",
    "# print(img_dcm)\n",
    "\n",
    "# # 查看 DICOM 图像\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.imshow(img_dcm, cmap='gray')\n",
    "# plt.savefig(\"./image/test20.png\")\n",
    "\n",
    "plt.imshow(img_nib, cmap='gray')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# # 获取当前文件夹中的所有.txt文件\n",
    "# txt_files = glob.glob(\"./image/*.png\")\n",
    "\n",
    "# # 删除每个.txt文件\n",
    "# for file in txt_files:\n",
    "#     os.remove(file)\n",
    "#     print(f\"{file} 已删除\")\n",
    "\n",
    "# #获取文件夹中所有的.pth文件\n",
    "# pth_files = glob.glob(\"./model_save/*.pth\")\n",
    "\n",
    "# for file in pth_files:\n",
    "#     os.remove(file)\n",
    "#     print(f\"{file} 已删除\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from load_img import load_dcm, load_nii\n",
    "import numpy as np\n",
    "from process_data import process_images_for_patients\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def process_images_for_patients(base_path, target_size,a =0):\n",
    "\n",
    "    # 获取所有病人文件夹\n",
    "    all_folders = sorted([f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))])\n",
    "\n",
    "    # 按病人分组文件夹，每两个文件夹为一个病人\n",
    "    patient_folders = []\n",
    "    for i in range(0, len(all_folders), 2):\n",
    "        \n",
    "        if i + 1 < len(all_folders):\n",
    "            patient_folders.append([all_folders[i], all_folders[i + 1]])\n",
    "\n",
    "    # print(len(patient_folders))\n",
    "\n",
    "    # for _, row in labels_df.iterrows():\n",
    "    #     label = row['N分期']  # 获取病人标签（N分期）\n",
    "        \n",
    "    # 加载每个文件夹中的图像\n",
    "    patient_images = []\n",
    "\n",
    "    for folder_pair in patient_folders:\n",
    "\n",
    "        all_images = []\n",
    "        for folder in folder_pair:\n",
    "            a += 1\n",
    "\n",
    "            dcm_file = None\n",
    "            nii_file = None\n",
    "\n",
    "            # print(folder)\n",
    "\n",
    "            # 查找对应的 .dcm 和 .nii 文件\n",
    "            for file in os.listdir(os.path.join(base_path, folder)):\n",
    "                if file.endswith('.dcm'):\n",
    "                    dcm_file = os.path.join(base_path, folder, file)\n",
    "                elif file.endswith('.nii.gz'):\n",
    "                    nii_file = os.path.join(base_path, folder, file)\n",
    "                           \n",
    "            # print(f\"dcm_file: {dcm_file}, nii_file: {nii_file}\")\n",
    "\n",
    "            if dcm_file and nii_file:\n",
    "                # 读取并处理 .dcm 和 .nii 图像\n",
    "                dcm_image = load_dcm(dcm_file)\n",
    "                nii_mask , top_left, bottom_right = load_nii(nii_file)\n",
    "\n",
    "                dcm_image = dcm_image[top_left[0]:bottom_right[0], top_left[1]:bottom_right[1]]\n",
    "\n",
    "                # 将两个图像相乘\n",
    "                focused_image = dcm_image * nii_mask\n",
    "\n",
    "                focused_image = cv2.resize(focused_image, target_size)\n",
    "                \n",
    "                # npimage = np.array(focused_dcm_image)\n",
    "                # np.savetxt(npimage,f'./txt/{a}.txt')\n",
    "                # print(npimage)\n",
    "                # np.savetxt(f\"./txt/{a}.txt\", npimage, fmt=\"%.1f\", delimiter=\",\")\n",
    "\n",
    "                # save_image(npimage,f\"./image/{a}.png\")\n",
    "                # plt.imshow(focused_dcm_image, cmap='gray')\n",
    "                # plt.savefig(f\"./image/{a}.png\")\n",
    "                \n",
    "\n",
    "                all_images.append(focused_image)\n",
    "\n",
    "            else:\n",
    "                dcm_image = load_dcm(dcm_file)\n",
    "                dcm_image = cv2.resize(dcm_image, target_size)\n",
    "                all_images.append(dcm_image)\n",
    "                        \n",
    "        if len(all_images) == 2:  # 确保每个病人有 2 张图像\n",
    "\n",
    "            # print(all_images[0].shape)\n",
    "            # 将两个图像堆叠在一起\n",
    "            patient_input = np.stack(all_images, axis=0)  # 形状为 (2, 512, 512)\n",
    "            # 追加至列表中\n",
    "            patient_images.append(patient_input)\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping patient {folder_pair} due to missing images\")\n",
    "\n",
    "    return patient_images\n",
    "\n",
    "def save_image(image_array, save_path):\n",
    "    \"\"\"\n",
    "    将灰度图像数组保存为图像文件\n",
    "    :param image_array: 灰度图像数组 (二维数组)\n",
    "    :param save_path: 保存路径\n",
    "    \"\"\"\n",
    "    # 确保数组是灰度图像（二维数组）\n",
    "    if len(image_array.shape) == 2:\n",
    "        # 将 NumPy 数组转换为 PIL 图像\n",
    "        img = Image.fromarray(image_array.astype(np.uint8))  # uint8 类型\n",
    "        img.save(save_path)\n",
    "    else:\n",
    "        print(len(image_array.shape))\n",
    "    \n",
    "\n",
    "base_path = './data/chaoyang_huigu'  # 图像数据的根目录\n",
    "save_path = './image'\n",
    "target_size = (224, 224)  # 目标图像尺寸\n",
    "\n",
    "patient_images = process_images_for_patients(base_path, target_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def cache_dataset(data, cache_path, format='npy'):\n",
    "    \"\"\"\n",
    "    缓存数据集到指定路径\n",
    "    \n",
    "    参数:\n",
    "    data: 要缓存的数据\n",
    "    cache_path: 缓存文件路径\n",
    "    format: 文件格式 ('npy', 'h5', 'pkl', 'joblib')\n",
    "    \"\"\"\n",
    "    cache_path = Path(cache_path)\n",
    "    \n",
    "    if format == 'npy':\n",
    "        np.save(cache_path, np.array(data))\n",
    "\n",
    "    elif format == 'pkl':\n",
    "        import pickle\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    elif format == 'joblib':\n",
    "        from joblib import dump\n",
    "        dump(data, cache_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {format}\")\n",
    "\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 缓存数据\n",
    "    cache_dataset(patient_images, 'cache/train.npy', format='npy')\n",
    "    \n",
    "    # 之后需要时加载数据\n",
    "    # patient_images = load_cached_dataset('cache/patient_images.npy', format='npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "# 加载标签数据\n",
    "def load_cached_dataset(cache_path, format='npy'):\n",
    "    \"\"\"\n",
    "    加载缓存的数据集\n",
    "    \n",
    "    参数:\n",
    "    cache_path: 缓存文件路径\n",
    "    format: 文件格式 ('npy', 'h5', 'pkl', 'joblib')\n",
    "    \"\"\"\n",
    "    cache_path = Path(cache_path)\n",
    "    \n",
    "    if not cache_path.exists():\n",
    "        raise FileNotFoundError(f\"Cache file not found: {cache_path}\")\n",
    "    \n",
    "    if format == 'npy':\n",
    "        return np.load(cache_path)\n",
    "\n",
    "    elif format == 'pkl':\n",
    "        import pickle\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    elif format == 'joblib':\n",
    "        from joblib import load\n",
    "        return load(cache_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {format}\")\n",
    "    \n",
    "patient_images = load_cached_dataset('cache/train.npy', format='npy')\n",
    "excel_path = './data/beiyou_excel/chaoyang_retrospective_233.xlsx'  # 包含病人姓名和标签的Excel文件路径\n",
    "labels_df = pd.read_excel(excel_path)\n",
    "\n",
    "# 补全标签并构建 images_with_labels 列表\n",
    "images_with_labels = []\n",
    "labels = []\n",
    "for i, patient_input in enumerate(patient_images):\n",
    "    label = labels_df.iloc[i]['N分期']  # 按顺序获取对应的标签\n",
    "\n",
    "    # 如果标签为 NaN，则用均值填充\n",
    "    if pd.isna(label):\n",
    "        label = 1.0\n",
    "\n",
    "    elif label == 2.0 or label == 3.0 :\n",
    "        label = 1.0\n",
    "    \n",
    "\n",
    "    images_with_labels.append((patient_input, label))\n",
    "    labels.append(label)\n",
    "\n",
    "# 输出处理后的标签\n",
    "# for i, (imageinput, label) in enumerate(images_with_labels):\n",
    "#     print(imageinput.shape)\n",
    "    # break\n",
    "    # print(f\"第 {i+1} 项标签: {label}\")\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "from dataset import ImageDataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "    # transforms.RandomVerticalFlip(),    # 随机垂直翻转\n",
    "    transforms.RandomRotation(20),      # 随机旋转 \n",
    "    # transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  \n",
    "    transforms.Normalize(mean=[0.5, 0.5], std=[0.5, 0.5]),  # 归一化到 [-1, 1]\n",
    "])\n",
    "\n",
    "# 验证集保持原始数据\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5], std=[0.5, 0.5]),  # 与训练集一致的归一化\n",
    "])\n",
    "\n",
    "\n",
    "# 创建数据集实例\n",
    "full_dataset = ImageDataset(images_with_labels)\n",
    "\n",
    "# # 使用 random_split 分割数据集\n",
    "# train_size = int(0.8 * len(full_dataset))  # 80% 训练集\n",
    "# val_size = len(full_dataset) - train_size  # 20% 验证集\n",
    "# train_subset, val_subset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# # 为分割后的子集添加数据增强\n",
    "# train_subset.dataset.transform = train_transform\n",
    "# val_subset.dataset.transform = val_transform\n",
    "\n",
    "# # 创建 DataLoader\n",
    "# train_loader = DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=0)\n",
    "# val_loader = DataLoader(val_subset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "# ## 检查训练集的第一个批次\n",
    "# for inputs, labels in train_loader:\n",
    "#     inputs_numpy = inputs.cpu().numpy()  # 如果在GPU上，先移动到CPU\n",
    "\n",
    "#     # 使用 numpy.array2string 以避免省略和更好地控制格式\n",
    "#     inputs_str = np.array2string(inputs_numpy, separator=', ', threshold=np.inf,precision=2)\n",
    "#     with open(\"output.txt\", 'w') as f:  # 'w' 表示每次打开文件时都会覆盖\n",
    "#         f.write(f\"输入张量的所有值:\\n{inputs_str}\\n\")  # 写入inputs的值\n",
    "\n",
    "#     # print(f\"输入张量形状: {inputs}\")\n",
    "#     print(f\"标签张量形状: {labels}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "from model import Resnet18_cbam\n",
    "\n",
    "# 假设 ImageDataset 和 Resnet18_cbam 已定义\n",
    "full_dataset = ImageDataset(images_with_labels)\n",
    "k_folds = 5\n",
    "batch_size = 4\n",
    "num_epochs = 30\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定义 KFold\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# 存储每个fold的结果\n",
    "fold_results = {}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(full_dataset)):\n",
    "    print(f'Fold {fold + 1}/{k_folds}')\n",
    "    \n",
    "    # 创建子集\n",
    "    train_subset = Subset(full_dataset, train_idx)\n",
    "    val_subset = Subset(full_dataset, val_idx)\n",
    "    \n",
    "    # 添加数据增强\n",
    "    train_subset.dataset.transform = train_transform\n",
    "    val_subset.dataset.transform = val_transform\n",
    "    \n",
    "    # 创建 DataLoader\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # 初始化模型、损失函数和优化器\n",
    "    model = Resnet18_cbam(num_classes=2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        try:\n",
    "            auc = roc_auc_score(all_labels, all_probs)\n",
    "        except ValueError:\n",
    "            auc = 0.5\n",
    "        print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}, AUC={auc:.4f}')\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'best_model_fold_{fold+1}.pth')\n",
    "    \n",
    "    fold_results[fold] = {'val_loss': best_val_loss}\n",
    "    print(f'Fold {fold + 1} 完成，最佳 Val Loss: {best_val_loss:.4f}\\n')\n",
    "\n",
    "# 输出所有fold的结果\n",
    "for fold in fold_results:\n",
    "    print(f'Fold {fold + 1} 最佳 Val Loss: {fold_results[fold][\"val_loss\"]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 训练函数\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # 将数据移动到 GPU 或 CPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # with open(\"debug_log.txt\", \"a\") as f:\n",
    "        #     f.write(f\"Inputs shape: {inputs.shape}\\n\")\n",
    "        #     f.write(f\"Labels shape: {labels.shape}\\n\")\n",
    "        #     f.write(f\"Inputs values: {inputs}\\n\")\n",
    "        #     f.write(f\"Labels values: {labels}\\n\")\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)  # 交叉熵损失函数\n",
    "        # loss = criterion(outputs, labels.view(-1, 1).float())  #   #需要确保标签形状为 (batch_size, 1)，并转换为 float 类型\n",
    "\n",
    "        # 反向传播与优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 记录损失\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 计算准确率\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        # total += labels.size(0)\n",
    "        # correct += (predicted.view(-1) == labels).sum().item()  \n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# 验证函数\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # 将数据移动到 GPU 或 CPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # print(f\"模型输出: {outputs[0]}\")\n",
    "            with open(\"test.txt\", \"a\") as f:\n",
    "                f.write(f\"{outputs}\\n\")\n",
    "\n",
    "            \n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            # loss = criterion(outputs, labels.view(-1, 1).float())\n",
    "\n",
    "            # 记录损失\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 计算准确率\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            # with open(\"test.txt\", \"a\") as f:\n",
    "            #     f.write(f\"{predicted}\\n\")\n",
    "\n",
    "            # total += labels.size(0)\n",
    "            # correct += (predicted.view(-1) == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "from model import Resnet18_cbam\n",
    "\n",
    "# 超参数设置\n",
    "model = Resnet18_cbam(num_classes=2)  \n",
    "num_epochs = 50\n",
    "learning_rate = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"使用设备：{device}\")\n",
    "\n",
    "# class_counts = [sum(label == i for label in labels) for i in range(2)]  # 每个类别的样本数量\n",
    "# class_weights = [1.0 / count for count in class_counts]  # 权重为样本数量的倒数\n",
    "# print(f\"每个类别的样本数量：{class_counts}\")\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# 模型、损失函数和优化器\n",
    "model = model.to(device)  # 将模型移动到 GPU 或 CPU\n",
    "criterion = nn.CrossEntropyLoss()  # 多分类交叉熵损失\n",
    "# criterion = nn.BCEWithLogitsLoss()  # 二分类交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # Adam 优化器\n",
    "\n",
    "# 存储训练和验证的结果\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    # 训练一个 epoch\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "\n",
    "    # 在验证集上评估\n",
    "    val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "    # 保存log\n",
    "    with open(\"log.txt\", \"a\") as f:\n",
    "        f.write(f\"Epoch {epoch + 1}/{num_epochs}\\n\")\n",
    "        f.write(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\\n\")\n",
    "        f.write(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\\n\")\n",
    "\n",
    "    # 保存验证集上准确率最高的模型\n",
    "    if epoch == 0 or val_acc > 0.65:\n",
    "        torch.save(model.state_dict(), f\"./model_save/epoch{epoch + 1}_model.pth\")\n",
    "        print(f\"Model saved at {epoch + 1}-th epoch\")\n",
    "\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(max(history[\"val_acc\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N分期（四分类）分类器,初步结果：\n",
    "\n",
    "数据预处理：将nii图像做距离变换，呈现聚焦效果，后与dcm图像做点乘\n",
    "输入：两张dcm图像组成的双通道，shape:(2,512,512)\n",
    "模型：Resnet18 + 注意力模块\n",
    "\n",
    "训练集+验证集：朝阳回顾_233\n",
    "验证集最大准确率：62%\n",
    "\n",
    "测试集：朝阳前瞻_190\n",
    "测试集最大准确率：52%\n",
    "AUC最大值：0.57\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
